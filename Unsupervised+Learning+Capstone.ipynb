{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import email\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#unique delim to separate columns without messing up content\n",
    "DELIMETER = chr(255)\n",
    "\n",
    "#needed because a few of the emails were too large for the default csv cell size\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#set paths (currently pathing for remote AWS EC2 Instance)\n",
    "root_path = '/home/ubuntu/ssl/notebooks/data/maildir'\n",
    "saved_file = '/home/ubuntu/ssl/notebooks/processed enron emails.csv'\n",
    "testing_batch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#steps through directory and loads every email in every folder into returned df\n",
    "def process_emails_into_df(origin, destination, testbool):\n",
    "    #safety check to see if the file has been created so I don't overwrite a finished file\n",
    "    already_processed = os.path.isfile(destination) \n",
    "\n",
    "    #if no file already exists start the writing process\n",
    "    if not already_processed:\n",
    "\n",
    "        #list for holding file paths\n",
    "        list_of_files = []    \n",
    "\n",
    "        #get the path for all the email files\n",
    "        for path, subdirs, files in os.walk(origin):\n",
    "            for name in files:\n",
    "                list_of_files.append(os.path.join(path, name))\n",
    "\n",
    "        email_count = len(list_of_files)\n",
    "        #caluculate 1% of the number of files, used for progress report printing.\n",
    "        onepcofemail = (email_count//100)\n",
    "\n",
    "\n",
    "\n",
    "        #create dataframe to store data to be written\n",
    "        emails = pd.DataFrame()\n",
    "        print('Starting. {}'.format(time.strftime(\"%I:%M:%S\")))\n",
    "\n",
    "        #counter for testing batches\n",
    "        x = 0\n",
    "\n",
    "        #iterrate through all the files found\n",
    "        for i, myfile in enumerate(list_of_files):\n",
    "\n",
    "            #if running a testing session to break before reading all file to inspect the saved csv\n",
    "            if x > 6000 and testbool: break\n",
    "\n",
    "            #read individual email file\n",
    "            with open(myfile, 'r', encoding='utf-8', errors='replace') as filepath:\n",
    "                #read in the email\n",
    "                message=email.message_from_string(filepath.read())\n",
    "\n",
    "                #save the important components\n",
    "                emails.loc[i,'Message-ID']=message['Message-ID']\n",
    "                emails.loc[i,'from']=message['from']\n",
    "                emails.loc[i,'subject']=message['subject']\n",
    "                emails.loc[i,'to']=message['to']\n",
    "                emails.loc[i,'cc']=message['cc']\n",
    "                emails.loc[i,'bcc']=message['Bcc']\n",
    "                emails.loc[i,'date']=message['date']\n",
    "                emails.loc[i,'file']=message['X-FileName']\n",
    "\n",
    "                #check that the body is one or multiple sections and save it\n",
    "                if message.is_multipart():\n",
    "                    string = ''\n",
    "                    for payload in message.get_payload():\n",
    "                        print(payload.get_payload())\n",
    "                        string = string + payload.get_payload()\n",
    "                        emails.loc[i,'body'] = string\n",
    "                else:\n",
    "                    emails.loc[i,'body'] =  message.get_payload()\n",
    "\n",
    "            #append entry to the csv file\n",
    "            with open(destination, 'a') as f:\n",
    "                #if this is the first entry use the headers, if not, dont.True\n",
    "                if i == 0: emails.to_csv(f, header=emails.columns, index = False, sep=DELIMETER)\n",
    "                else: emails.to_csv(f, header=None, index = False, sep=DELIMETER)\n",
    "\n",
    "            #clear the dataframe to save memory\n",
    "            emails = pd.DataFrame()        \n",
    "\n",
    "            #print statement updating progress report\n",
    "            if i % onepcofemail == 0: \n",
    "                print('{}% finished. {}'.format(i//onepcofemail, time.strftime(\"%I:%M:%S\")), end=\"\\r\")\n",
    "\n",
    "            #itterate counter for testing batches\n",
    "            x = x+1\n",
    "\n",
    "\n",
    "        #process complete\n",
    "        print('Done! Exported {} lines to CSV {}'.format(x, time.strftime(\"%I:%M:%S\")))\n",
    "\n",
    "    #prints if the file already exists and there is no need to process the individual emails into a csv\n",
    "    else: \n",
    "        #open csvs of all emails\n",
    "        print('**********There is already a Process CSV of All Emails, loading Now************', end=\"\\r\")\n",
    "        all_emails = pd.read_csv(saved_file, sep=DELIMETER, engine='python')\n",
    "        print('***Processed Emails into DF Loaded From CSV, Delete File to Process Again******')\n",
    "        return all_emails        \n",
    "\n",
    "#function that filters out non enron originating email addresses       \n",
    "def filter_unwanted_senders(dataframe, new_file_dest):\n",
    "    path='/home/ubuntu/ssl/notebooks/' + new_file_dest\n",
    "    \n",
    "    #check to see if file has already been processed and saved\n",
    "    if not os.path.isfile(path): \n",
    "        \n",
    "        #get a list of unique email senders\n",
    "        unique_senders_inc_small = dataframe['from'].unique()\n",
    "        \n",
    "        #create an empty df to holde data\n",
    "        filtered_df = pd.DataFrame(columns = dataframe.columns)\n",
    "        \n",
    "        #iterate through all the unique senders\n",
    "        for i, sender in enumerate(unique_senders_inc_small):\n",
    "            temp = dataframe[dataframe['from'] == sender]\n",
    "            \n",
    "            #prints progress report\n",
    "            print('{}% finished filtering unwanted senders            '.format(round(i/len(unique_senders_inc_small)*100,2)) , end=\"\\r\")\n",
    "\n",
    "            #tests and saves senders who have enron emails and have sent more than 50 emails\n",
    "            if '@enron.com' in sender:\n",
    "                filtered_df = filtered_df.append(temp, ignore_index = True)\n",
    "        \n",
    "        #save new list of senders\n",
    "        filtered_df.to_csv(path, sep=DELIMETER, index = False)\n",
    "    \n",
    "    #if it has already been done don't do it again\n",
    "    else: \n",
    "        print('*********Previously Filtered Senders List. Loading Old File!*********'.format(new_file_dest), end=\"\\r\")\n",
    "        filtered_df = pd.read_csv(path, sep=DELIMETER, engine='python')\n",
    "        print('****To Reprocess Filtered Senders Delete {}.  Loaded Old File!*******'.format(new_file_dest))\n",
    "    filtered_df = filtered_df.dropna(subset = ['to', 'cc', 'bcc'], how = 'all')\n",
    "    filtered_df = filtered_df.dropna(subset = ['body'])    \n",
    "    return filtered_df\n",
    "\n",
    "#makes a list of all senders and a lists all the recipients they send to in next column\n",
    "def get_sender_recip_list(dataframe):\n",
    "    new_file = '/home/ubuntu/ssl/notebooks/sender_recip_df_list'\n",
    "    \n",
    "    if not os.path.isfile(new_file):\n",
    "    \n",
    "        #empty df for leading up addresses\n",
    "        sender_by_recip = pd.DataFrame(columns = ['sender','recipients', 'booksize'])\n",
    "\n",
    "        #list of unique senders\n",
    "        unique_senders = dataframe['from'].unique()\n",
    "\n",
    "        #arrays for holding values to be stored\n",
    "        senders = []\n",
    "        recipients = []\n",
    "        addressbooksize = []\n",
    "\n",
    "        #itterate through each unique email writer\n",
    "        for i, sender in enumerate(unique_senders):\n",
    "\n",
    "            #make a list of the samples from this itterations sender\n",
    "            users_emails = dataframe[dataframe['from'] == sender]\n",
    "\n",
    "            all_recips = users_emails['to'].tolist() + users_emails['cc'].tolist() + users_emails['bcc'].tolist() \n",
    "\n",
    "\n",
    "\n",
    "            senders.append(sender)\n",
    "\n",
    "            namelist = []\n",
    "            for l in all_recips:\n",
    "                if not isinstance(l, str): pass\n",
    "                else: \n",
    "                    l = l.replace('\\t','').replace('\\n','')\n",
    "                    list_of_names = l.split(',')\n",
    "                    for name in list_of_names:\n",
    "                        if '<' in name:\n",
    "                            result = re.search('<(.*)>', name)\n",
    "                            name = result.group(1)\n",
    "                            name = name.replace('\"', '')\n",
    "                            name = name.replace(\"'\", '')\n",
    "                            namelist.append(name)\n",
    "                        #unique typing of someones email incorrectly, ignore it\n",
    "                        elif name == '\"john\\\".<john.dunn\"@enron@enron.com': pass\n",
    "                        else:\n",
    "                            name = name.replace('\"', '')\n",
    "                            name = name.replace(\"'\", '')\n",
    "                            namelist.append(name)\n",
    "\n",
    "\n",
    "            #name = [item for sublist in name for item in sublist]\n",
    "            unique_recips = list(set(namelist))\n",
    "            recipients.append(unique_recips)\n",
    "\n",
    "            addressbooksize.append(len(unique_recips))\n",
    "\n",
    "            print('{}% of sender + recip dataframe completed        '.format(round(i/len(unique_senders)*100,2)) , end=\"\\r\")\n",
    "\n",
    "        sender_by_recip['sender'] = senders\n",
    "        sender_by_recip['recipients'] = recipients\n",
    "        sender_by_recip['booksize'] = addressbooksize\n",
    "        print('********************* Sender Recip Dataframe Completed ******************')\n",
    "        sender_by_recip.to_csv(new_file, sep=DELIMETER, index = False)\n",
    "        return sender_by_recip\n",
    "    else:\n",
    "        print('********************* Sender Recip Already Processed ********************', end=\"\\r\")\n",
    "        sender_by_recip = pd.read_csv(new_file, sep=DELIMETER, engine = 'python')\n",
    "        print('************ Sender Recip Dataframe with Lists Already Loaded ***********')\n",
    "        return sender_by_recip\n",
    "\n",
    "def shrink_addressbooks(sender_recip_df, low, high):\n",
    "    sender_recip_df = sender_recip_df[sender_recip_df['booksize']>low]\n",
    "    sender_recip_df = sender_recip_df[sender_recip_df['booksize']<high]\n",
    "    return sender_recip_df\n",
    "\n",
    "#generates a dataframe with every sender and one of their recipients in each row\n",
    "def list_of_s_r_combos(addressbook, emails):\n",
    "    if not os.path.isfile('/home/ubuntu/ssl/notebooks/list_of_s_r'):\n",
    "        unique_senders = addressbook['sender'].unique()\n",
    "\n",
    "        senders = []\n",
    "        recipients = []\n",
    "        list_of_bodies = []\n",
    "        s_r_combos = pd.DataFrame(columns = ['sender', 'recipient'])\n",
    "        for i, sender in enumerate(unique_senders):\n",
    "            senders_recips = addressbook[addressbook['sender'] == sender].recipients\n",
    "            senders_recips = [item for sublist in senders_recips for item in sublist]\n",
    "            for j, name in enumerate(senders_recips):\n",
    "                recip = name.strip(' ')\n",
    "                senders.append(sender)\n",
    "                recipients.append(recip)\n",
    "\n",
    "        s_r_combos['sender'] = senders\n",
    "        s_r_combos['recipient'] = recipients\n",
    "        print('Processed All Sender/Reciever Combos')           \n",
    "        \n",
    "        s_r_combos.to_csv('/home/ubuntu/ssl/notebooks/list_of_s_r', index = False)\n",
    "        print('***************Sender/Reciever Pair DataFrame Processed ***************.')\n",
    "    else: \n",
    "        print('**********Send/Recieve Pair DF already processed. Loading CSV!**********', end=\"\\r\")\n",
    "        s_r_combos = pd.read_csv('/home/ubuntu/ssl/notebooks/list_of_s_r')\n",
    "        print('*****Send/Recieve Pair CSV loaded! Delete list_of_s_r to reprocess******')\n",
    "    return s_r_combos\n",
    "\n",
    "#calculate the mean tf_idf score for a series of texts\n",
    "def tfidf(txt, vectorizer):\n",
    "    try: \n",
    "        X = vectorizer.fit_transform(txt)\n",
    "        Y = list(X.tocoo().data)\n",
    "        Y = [i/sum(Y) for i in Y]\n",
    "    except ValueError:\n",
    "        Y = [0]\n",
    "    return Y\n",
    "\n",
    "#add the tf_idf score to a dataframe listing senders paired with recipients from a dataframe of emails\n",
    "def add_tf_score(combo_list, emails):\n",
    "    \n",
    "    #saved file location\n",
    "    new_file = '/home/ubuntu/ssl/notebooks/scored_w_mean_idf'\n",
    "    \n",
    "    #if already completed calculations before do not recalculate or overwrite file\n",
    "    if not os.path.isfile(new_file):\n",
    "        \n",
    "        #list for holding mean tf_idf scores\n",
    "        tf_idf_body =[]\n",
    "        tfidfv = TfidfVectorizer(min_df=1)\n",
    "        \n",
    "        #iterate through all sender/recipient combos in the dataframe\n",
    "        for index, row in combo_list.iterrows():\n",
    "            #current sender\n",
    "            from_ = row['sender']\n",
    "            #current recipient\n",
    "            to_ = row['recipient']\n",
    "            \n",
    "            #select all eamil bodies from and to current selection\n",
    "            combo_s_r = emails[(emails['from']== from_) & \n",
    "                                     ((emails['to'].str.contains(to_)) | \n",
    "                                      (emails['cc'].str.contains(to_) | \n",
    "                                      (emails['bcc'].str.contains(to_))))]['body']\n",
    "            \n",
    "            #calculate mean tf_idf score\n",
    "            tf_idf_mean = tfidf(combo_s_r, tfidfv)\n",
    "            \n",
    "            #append score to list\n",
    "            tf_idf_body.append(tf_idf_mean)\n",
    "\n",
    "            #print status updates\n",
    "            print('{} % done Time: {}                   '.format((round(index/len(combo_list),4)*100),time.strftime(\"%I:%M:%S\")), end=\"\\r\")\n",
    "\n",
    "\n",
    "        #append tf_idf scores to dataframe in new column\n",
    "        combo_list['tf_idf_mean'] = tf_idf_body\n",
    "        \n",
    "        #save file with unique delimeter\n",
    "        combo_list.to_csv(new_file, sep=DELIMETER)\n",
    "        return combo_list   \n",
    "    else:\n",
    "        \n",
    "        #let user know process was previously completed and open previously processed file\n",
    "        print('Scores were previously calculated.  Opening saved File')\n",
    "        combo_list = pd.read_csv(new_file, sep=DELIMETER, engine = 'python')\n",
    "        print('*********TF_IDF scores file Opened *******************')\n",
    "        return combo_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Processed Emails into DF Loaded From CSV, Delete File to Process Again******\n",
      "****To Reprocess Filtered Senders Delete small_senders_filtered.  Loaded Old File!*******\n",
      "************ Sender Recip Dataframe with Lists Already Loaded ***********\n",
      "*****Send/Recieve Pair CSV loaded! Delete list_of_s_r to reprocess******\n"
     ]
    }
   ],
   "source": [
    "#read in all files to process emails into dataframe\n",
    "all_emails = process_emails_into_df(root_path, saved_file, testing_batch)\n",
    "\n",
    "#filter out the non enron and make a list of their email addresses\n",
    "#and the people they sent emails to\n",
    "wanted_emails = filter_unwanted_senders(all_emails, 'small_senders_filtered')\n",
    "\n",
    "#split data into train and test sets\n",
    "X_train, X_test = train_test_split(wanted_emails, test_size= 0.25)\n",
    "\n",
    "#compile a dataframe of all senders and all unique recipients in second column\n",
    "senders_to = get_sender_recip_list(X_train)\n",
    "\n",
    "#eliminate senders who have fewer than low or more than high number of recipients\n",
    "smaller_book = shrink_addressbooks(senders_to, low = 50, high = 300)\n",
    "\n",
    "#parse lists of recipients into unique entrys, each row has 1 sender and 1 reciever\n",
    "list_of_s_r = list_of_s_r_combos(smaller_book, wanted_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del all_emails\n",
    "del senders_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 04:45:58\n",
      "0.04 % done Time: 04:46:49                   \r"
     ]
    }
   ],
   "source": [
    "print('Starting {}'.format(time.strftime(\"%I:%M:%S\")))\n",
    "scores = add_tf_score(list_of_s_r, wanted_emails)\n",
    "print('Finished {}'.format(time.strftime(\"%I:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
