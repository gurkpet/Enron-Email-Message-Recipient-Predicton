{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import email\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "#unique delim to separate columns without messing up content\n",
    "DELIMETER = chr(255)\n",
    "\n",
    "#needed because a few of the emails were too large for the default csv cell size\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#set paths (currently pathing for remote AWS EC2 Instance)\n",
    "root_path = '/home/ubuntu/ssl/notebooks/data/maildir'\n",
    "saved_file = '/home/ubuntu/ssl/notebooks/processed enron emails.csv'\n",
    "testing_batch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#steps through directory and loads every email in every folder into returned df\n",
    "def process_emails_into_df(origin, destination, testbool):\n",
    "    #safety check to see if the file has been created so I don't overwrite a finished file\n",
    "    already_processed = os.path.isfile(destination) \n",
    "\n",
    "    #if no file already exists start the writing process\n",
    "    if not already_processed:\n",
    "\n",
    "        #list for holding file paths\n",
    "        list_of_files = []    \n",
    "\n",
    "        #get the path for all the email files\n",
    "        for path, subdirs, files in os.walk(origin):\n",
    "            for name in files:\n",
    "                list_of_files.append(os.path.join(path, name))\n",
    "\n",
    "        email_count = len(list_of_files)\n",
    "        #caluculate 1% of the number of files, used for progress report printing.\n",
    "        onepcofemail = (email_count//100)\n",
    "\n",
    "\n",
    "\n",
    "        #create dataframe to store data to be written\n",
    "        emails = pd.DataFrame()\n",
    "        print('Starting. {}'.format(time.strftime(\"%I:%M:%S\")))\n",
    "\n",
    "        #counter for testing batches\n",
    "        x = 0\n",
    "\n",
    "        #iterrate through all the files found\n",
    "        for i, myfile in enumerate(list_of_files):\n",
    "\n",
    "            #if running a testing session to break before reading all file to inspect the saved csv\n",
    "            if x > 6000 and testbool: break\n",
    "\n",
    "            #read individual email file\n",
    "            with open(myfile, 'r', encoding='utf-8', errors='replace') as filepath:\n",
    "                #read in the email\n",
    "                message=email.message_from_string(filepath.read())\n",
    "\n",
    "                #save the important components\n",
    "                emails.loc[i,'Message-ID']=message['Message-ID']\n",
    "                emails.loc[i,'from']=message['from']\n",
    "                emails.loc[i,'subject']=message['subject']\n",
    "                emails.loc[i,'to']=message['to']\n",
    "                emails.loc[i,'cc']=message['cc']\n",
    "                emails.loc[i,'bcc']=message['Bcc']\n",
    "                emails.loc[i,'date']=message['date']\n",
    "                emails.loc[i,'file']=message['X-FileName']\n",
    "\n",
    "                #check that the body is one or multiple sections and save it\n",
    "                if message.is_multipart():\n",
    "                    string = ''\n",
    "                    for payload in message.get_payload():\n",
    "                        print(payload.get_payload())\n",
    "                        string = string + payload.get_payload()\n",
    "                        emails.loc[i,'body'] = string\n",
    "                else:\n",
    "                    emails.loc[i,'body'] =  message.get_payload()\n",
    "\n",
    "            #append entry to the csv file\n",
    "            with open(destination, 'a') as f:\n",
    "                #if this is the first entry use the headers, if not, dont.True\n",
    "                if i == 0: emails.to_csv(f, header=emails.columns, index = False, sep=DELIMETER)\n",
    "                else: emails.to_csv(f, header=None, index = False, sep=DELIMETER)\n",
    "\n",
    "            #clear the dataframe to save memory\n",
    "            emails = pd.DataFrame()        \n",
    "\n",
    "            #print statement updating progress report\n",
    "            if i % onepcofemail == 0: \n",
    "                print('{}% finished. {}'.format(i//onepcofemail, time.strftime(\"%I:%M:%S\")), end=\"\\r\")\n",
    "\n",
    "            #itterate counter for testing batches\n",
    "            x = x+1\n",
    "\n",
    "\n",
    "        #process complete\n",
    "        print('Done! Exported {} lines to CSV {}'.format(x, time.strftime(\"%I:%M:%S\")))\n",
    "\n",
    "    #prints if the file already exists and there is no need to process the individual emails into a csv\n",
    "    else: \n",
    "        #open csvs of all emails\n",
    "        print('**********There is already a Process CSV of All Emails, loading Now************', end=\"\\r\")\n",
    "        all_emails = pd.read_csv(saved_file, sep=DELIMETER, engine='python')\n",
    "        print('***Processed Emails into DF Loaded From CSV, Delete File to Process Again******')\n",
    "        return all_emails        \n",
    "\n",
    "#function that filters out non enron originating email addresses       \n",
    "def filter_unwanted_senders(dataframe, new_file_dest):\n",
    "    path='/home/ubuntu/ssl/notebooks/' + new_file_dest\n",
    "    \n",
    "    #check to see if file has already been processed and saved\n",
    "    if not os.path.isfile(path): \n",
    "        \n",
    "        #get a list of unique email senders\n",
    "        unique_senders_inc_small = dataframe['from'].unique()\n",
    "        \n",
    "        #create an empty df to holde data\n",
    "        filtered_df = pd.DataFrame(columns = dataframe.columns)\n",
    "        \n",
    "        #iterate through all the unique senders\n",
    "        for i, sender in enumerate(unique_senders_inc_small):\n",
    "            temp = dataframe[dataframe['from'] == sender]\n",
    "            \n",
    "            #prints progress report\n",
    "            print('{}% finished filtering unwanted senders            '.format(round(i/len(\n",
    "                                                                            unique_senders_inc_small)\n",
    "                                                                                     *100,2)) , end=\"\\r\")\n",
    "\n",
    "            #tests and saves senders who have enron emails\n",
    "            if '@enron.com' in sender:\n",
    "                filtered_df = filtered_df.append(temp, ignore_index = True)\n",
    "        \n",
    "        #save new list of senders\n",
    "        filtered_df.to_csv(path, sep=DELIMETER, index = False)\n",
    "    \n",
    "    #if it has already been done don't do it again\n",
    "    else: \n",
    "        print('*********Previously Filtered Senders List. Loading Old File!*********'.format(new_file_dest),\n",
    "                                                                                              end=\"\\r\")\n",
    "        filtered_df = pd.read_csv(path, sep=DELIMETER, engine='python')\n",
    "        print('****To Reprocess Filtered Senders Delete {}.  Loaded Old File!*******'.format(new_file_dest))\n",
    "    filtered_df = filtered_df.dropna(subset = ['to', 'cc', 'bcc'], how = 'all')\n",
    "    filtered_df = filtered_df.dropna(subset = ['body'])    \n",
    "    return filtered_df\n",
    "\n",
    "#makes a list of all senders and a lists all the recipients they send to in next column\n",
    "def get_sender_recip_list(dataframe):\n",
    "    new_file = '/home/ubuntu/ssl/notebooks/sender_recip_df_list'\n",
    "    \n",
    "    if not os.path.isfile(new_file):\n",
    "    \n",
    "        #empty df for leading up addresses\n",
    "        sender_by_recip = pd.DataFrame(columns = ['sender','recipient', 'booksize'])\n",
    "\n",
    "        #list of unique senders\n",
    "        unique_senders = dataframe['from'].unique()\n",
    "\n",
    "        #arrays for holding values to be stored\n",
    "        senders = []\n",
    "        recipients = []\n",
    "        addressbooksize = []\n",
    "\n",
    "        #itterate through each unique email writer\n",
    "        for i, sender in enumerate(unique_senders):\n",
    "\n",
    "            #make a list of the samples from this itterations sender\n",
    "            users_emails = dataframe[dataframe['from'] == sender]\n",
    "\n",
    "            all_recips = users_emails['to'].tolist() + users_emails['cc'].tolist() + users_emails['bcc'].tolist() \n",
    "\n",
    "            if '<' in sender:\n",
    "                sender = re.search('<.(.*)>', sender)\n",
    "                sender = sender.group(1)\n",
    "            \n",
    "\n",
    "            namelist = []\n",
    "            for l in all_recips:\n",
    "                if not isinstance(l, str): pass\n",
    "                else: \n",
    "                    l = l.replace('\\t','').replace('\\n','')\n",
    "                    list_of_names = l.split(',')\n",
    "                    for name in list_of_names:\n",
    "                        \n",
    "                        if '<' in name:\n",
    "                            result = re.search('<.(.*)>', name)\n",
    "                            name = result.group(1)\n",
    "                            name = name.replace('\"', '')\n",
    "                            name = name.replace(\"'\", '')\n",
    "                            name = name.replace(\"..\", '.')\n",
    "                            name = name.strip(' ')\n",
    "                            namelist.append(name)\n",
    "                        \n",
    "                        else:\n",
    "                            name = name.replace('\"', '')\n",
    "                            name = name.replace(\"'\", '')\n",
    "                            name = name.strip(' ')\n",
    "                            name = name.replace(\"..\", '.')\n",
    "                            namelist.append(name)\n",
    "                        \n",
    "\n",
    "\n",
    "            #name = [item for sublist in name for item in sublist]\n",
    "            unique_recips = list(set(namelist))\n",
    "            for name in unique_recips:\n",
    "                recipients.append(name)\n",
    "                senders.append(sender)\n",
    "                addressbooksize.append(len(unique_recips))\n",
    "            \n",
    "\n",
    "            print('{}% of sender + recip dataframe completed        '.format(\n",
    "                                                                        round(i/len(unique_senders)*100,2))\n",
    "                                                                        , end=\"\\r\")\n",
    "\n",
    "        sender_by_recip['sender'] = senders\n",
    "        sender_by_recip['recipient'] = recipients\n",
    "        sender_by_recip['booksize'] = addressbooksize\n",
    "        print('********************* Sender Recip Dataframe Completed ******************')\n",
    "        sender_by_recip.to_csv(new_file, sep=DELIMETER, index = False)\n",
    "        return sender_by_recip\n",
    "    else:\n",
    "        print('********************* Sender Recip Already Processed ********************', end=\"\\r\")\n",
    "        sender_by_recip = pd.read_csv(new_file, sep=DELIMETER, engine = 'python')\n",
    "        print('************ Sender Recip Dataframe with Lists CSV Loaded ***********')\n",
    "        return sender_by_recip\n",
    "\n",
    "#calculate the mean tf_idf score for a series of texts\n",
    "def tfidf(txt, vectorizer):\n",
    "    try: \n",
    "        X = vectorizer.fit_transform(txt)\n",
    "        Y = list(X.tocoo().data)\n",
    "        Y = [i/sum(Y) for i in Y]\n",
    "    except ValueError:\n",
    "        Y = [0]\n",
    "    return Y\n",
    "\n",
    "#add the tf_idf score to a dataframe listing senders paired with recipients from a dataframe of emails\n",
    "def add_tf_score(combo_list, emails):\n",
    "    \n",
    "    #saved file location\n",
    "    new_file = '/home/ubuntu/ssl/notebooks/scored_w_mean_idf'\n",
    "    \n",
    "    #if already completed calculations before do not recalculate or overwrite file\n",
    "    if not os.path.isfile(new_file):\n",
    "        \n",
    "        #list for holding mean tf_idf scores\n",
    "        senders = []\n",
    "        recievers = []\n",
    "        tf_idf_body =[]\n",
    "        tfidfv = TfidfVectorizer(min_df=1)\n",
    "        \n",
    "        #iterate through all sender/recipient combos in the dataframe\n",
    "        for index, row in combo_list.iterrows():\n",
    "            #current sender\n",
    "            from_ = row['sender']\n",
    "            #current recipient\n",
    "            to_ = row['recipient']\n",
    "            \n",
    "            #select all eamil bodies from and to current selection\n",
    "            combo_s_r = emails[(emails['from']== from_) & ((emails['to'].str.contains(to_)) | (emails['cc'].str.contains(to_)) | (emails['bcc'].str.contains(to_)))]['body']\n",
    "            \n",
    "            if len(combo_s_r) > 100:\n",
    "                combo_s_r = combo_s_r[:100]\n",
    "            \n",
    "            if len(combo_s_r) > 50:\n",
    "                print('{}% done Time: {}, From: {}, To: {}. Entries: {}                            '.format((round(index/\n",
    "                                                                len(combo_list),4)*100),\n",
    "                                                                time.strftime(\"%I:%M:%S\"), from_, to_,len(combo_s_r)), end=\"\\r\")\n",
    "\n",
    "                #calculate mean tf_idf score\n",
    "                tf_idf_mean = tfidf(combo_s_r, tfidfv)\n",
    "\n",
    "                #append score to list\n",
    "                senders.append(from_)\n",
    "                recievers.append(to_)\n",
    "                tf_idf_body.append(tf_idf_mean)\n",
    "\n",
    "\n",
    "\n",
    "        #append tf_idf scores to dataframe in new column\n",
    "        calced_tfidf_scores = pd.DataFrame()\n",
    "        calced_tfidf_scores['from'] = from_\n",
    "        calced_tfidf_scores['to'] = to_\n",
    "        calced_tfidf_scores['tf_idf_mean'] = tf_idf_body\n",
    "        \n",
    "        #save file with unique delimeter\n",
    "        calced_tfidf_scores.to_csv(new_file, sep=DELIMETER, index = False)\n",
    "        return calced_tfidf_scores   \n",
    "    else:\n",
    "        \n",
    "        #let user know process was previously completed and open previously processed file\n",
    "        print('Scores were previously calculated.  Opening saved File')\n",
    "        calced_tfidf_scores = pd.read_csv(new_file, sep=DELIMETER, engine = 'python')\n",
    "        print('*********TF_IDF scores file Opened *******************')\n",
    "        return calced_tfidf_scores\n",
    "\n",
    "\n",
    "def remove_strang_problematic_addesses(s_r_pair_list):\n",
    "    \n",
    "    #this one was VERY problematic to parse, maybe best to skip this sender\n",
    "    s_r_pair_list = s_r_pair_list[s_r_pair_list['sender'] != 'enron.announcements@enron.com']\n",
    "    \n",
    "    s_r_pair_list = s_r_pair_list[s_r_pair_list['sender'] != '40enron@enron.com']\n",
    "\n",
    "    #it hung on this one for hours so lets skip it\n",
    "    bad_index = s_r_pair_list[((s_r_pair_list['sender'] == 'miyung.buster@enron.com') & \n",
    "                (s_r_pair_list['recipient'] == 'john.shelk@enron.com'))].index\n",
    "    \n",
    "    #do the actual dropping\n",
    "    s_r_pair_list.drop(bad_index, inplace = True)\n",
    "    del bad_index\n",
    "    \n",
    "    #there were some junk addresses that were clearly just in error, lets drop those\n",
    "    bad_index = s_r_pair_list[s_r_pair_list['recipient'].str.contains('@enron@enron.com')].index\n",
    "\n",
    "    #do the actual dropping\n",
    "    s_r_pair_list.drop(bad_index, inplace = True)\n",
    "    del bad_index\n",
    "    \n",
    "    #there were some junk addresses that were clearly just in error, lets drop those\n",
    "    bad_index = s_r_pair_list[s_r_pair_list['recipient'].str.contains('/')].index\n",
    "\n",
    "    #do the actual dropping\n",
    "    s_r_pair_list.drop(bad_index, inplace = True)\n",
    "    del bad_index\n",
    "    \n",
    "    #there were some junk addresses that were clearly just in error, lets drop those\n",
    "    bad_index = s_r_pair_list[s_r_pair_list['recipient'].str.contains('&')].index\n",
    "\n",
    "    #do the actual dropping\n",
    "    s_r_pair_list.drop(bad_index, inplace = True)\n",
    "    del bad_index\n",
    "    \n",
    "    #there were some junk addresses that were clearly just in error, lets drop those\n",
    "    bad_index = s_r_pair_list[s_r_pair_list['recipient'] == '<'].index\n",
    "\n",
    "    #do the actual dropping\n",
    "    s_r_pair_list.drop(bad_index, inplace = True)\n",
    "    del bad_index\n",
    "    \n",
    "    #reset index values after removing elements from the process\n",
    "    s_r_pair_list.index = range(len(s_r_pair_list.index))\n",
    "    \n",
    "    #only deal with recievers with enron.com email addresses\n",
    "    s_r_pair_list = s_r_pair_list[s_r_pair_list['recipient'].str.contains('@enron.com')]\n",
    "    \n",
    "    #get rid of peoples typos sending to @enron.com\n",
    "    s_r_pair_list = s_r_pair_list[s_r_pair_list['recipient'] != '@enron.com']\n",
    "    \n",
    "    return s_r_pair_list\n",
    "\n",
    "def email_exchange_count(pair_list, emails):\n",
    "    new_list = pd.DataFrame()\n",
    "    new_file = '/home/ubuntu/ssl/notebooks/emailcountsadded'\n",
    "    \n",
    "    if not os.path.isfile(new_file):\n",
    "        for index, row in pair_list.iterrows():\n",
    "                #current sender\n",
    "                from_ = row['sender']\n",
    "                #current recipient\n",
    "                to_ = row['recipient']\n",
    "\n",
    "                #select all eamil bodies from and to current selection\n",
    "                combo_s_r = emails[(emails['from']== from_) & ((emails['to'].str.contains(to_)) | (emails['cc'].str.contains(to_)) | (emails['bcc'].str.contains(to_)))]['body']\n",
    "\n",
    "                new_list['sender'] = from_\n",
    "                new_list['recipient'] = to_\n",
    "                new_list['addressbooksize'] = row['booksize']\n",
    "                new_list['num_emails'] = len(combo_s_r)\n",
    "\n",
    "                print('{}% done Time: {}, From: {}, To: {}. Entries: {}                            '.format((round(index/\n",
    "                                                                    len(pair_list),4)*100),\n",
    "                                                                    time.strftime(\"%I:%M:%S\"), from_, to_,len(combo_s_r)), end=\"\\r\")\n",
    "        new_list.to_csv(new_file, sep=DELIMETER, index = False)\n",
    "        return new_list\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #let user know process was previously completed and open previously processed file\n",
    "        print('Counts were previously calculated.  Opening saved File')\n",
    "        new_list = pd.read_csv(new_file, sep=DELIMETER, engine = 'python')\n",
    "        print('*********EmailCounts  file Opened *******************')\n",
    "        return new_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Processed Emails into DF Loaded From CSV, Delete File to Process Again******\n",
      "****To Reprocess Filtered Senders Delete small_senders_filtered.  Loaded Old File!*******\n",
      "************ Sender Recip Dataframe with Lists CSV Loaded ***************\n",
      "Starting 12:02:42\n",
      "2.9899999999999998% done Time: 12:41:33, From: no.address@enron.com, To: phillip.foster@enron.com. Entries: 1                                              \r"
     ]
    }
   ],
   "source": [
    "#read in all files to process emails into dataframe\n",
    "all_emails = process_emails_into_df(root_path, saved_file, testing_batch)\n",
    "\n",
    "#filter out the non enron email senders\n",
    "wanted_emails = filter_unwanted_senders(all_emails, 'small_senders_filtered')\n",
    "\n",
    "#compile a dataframe of all senders and all unique recipients to that sender\n",
    "senders_to = get_sender_recip_list(wanted_emails)\n",
    "\n",
    "#some final cleaning of addresses that cause problems or that are clearly just junk\n",
    "clean_pair_list = remove_strang_problematic_addesses(senders_to)\n",
    "\n",
    "#count the number of emails sent from each sender to each recipient\n",
    "smaller_sr_list = email_exchange_count(clean_pair_list, all_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smaller_sr_list = smaller_sr_list[smaller_sr_list['num_emails'] > 50]\n",
    "\n",
    "len(smaller_sr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(wanted_emails, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting {}'.format(time.strftime(\"%I:%M:%S\")))\n",
    "scores = add_tf_score(clean_pair_list, X_train)\n",
    "print('Finished {}'.format(time.strftime(\"%I:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_tf_score_v2(combo_list, emails):\n",
    "    \n",
    "    #saved file location\n",
    "    new_file = '/home/ubuntu/ssl/notebooks/scored_w_mean_idf'\n",
    "    \n",
    "    #if already completed calculations before do not recalculate or overwrite file\n",
    "    if not os.path.isfile(new_file):\n",
    "        \n",
    "        #list for holding mean tf_idf scores\n",
    "        tf_idf_body =[]\n",
    "        \n",
    "        \n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        #append tf_idf scores to dataframe in new column\n",
    "        if __name__ == \"__main__\":\n",
    "            combo_list['tf_idf_mean'] = Parallel(n_jobs=num_cores)(delayed(\n",
    "                looping_vectorizor(emails + combo_list + tf_idf_body)) for row in combo_list.iterrows())\n",
    "            \n",
    "        #save file with unique delimeter\n",
    "        print('*************************Saving TF_IDF scores File*****************************', end=\"\\r\")\n",
    "        combo_list.to_csv(new_file, sep=DELIMETER, index = False)\n",
    "        print('************************TF_IDF scores File Saved ******************************')\n",
    "        return combo_list   \n",
    "    else:\n",
    "        \n",
    "        #let user know process was previously completed and open previously processed file\n",
    "        print('************Scores were previously calculated.  Opening saved File**************', end=\"\\r\")\n",
    "        combo_list = pd.read_csv(new_file, sep=DELIMETER, engine = 'python')\n",
    "        print('************************TF_IDF scores File Opened ******************************')\n",
    "        return combo_list\n",
    "\n",
    "def looping_vectorizor(emailslist, combination_s_r, tf_idf_body):\n",
    "    #iterate through all sender/recipient combos in the dataframe\n",
    "    \n",
    "    for index, row in combo_list.iterrows():\n",
    "            #current sender\n",
    "            from_ = row['sender']\n",
    "            #current recipient\n",
    "            to_ = row['recipient']\n",
    "            \n",
    "            #select all eamil bodies from and to current selection\n",
    "            combo_s_r = emails[(emails['from']== from_) & \n",
    "                                     ((emails['to'].str.contains(to_)) | \n",
    "                                      (emails['cc'].str.contains(to_) | \n",
    "                                      (emails['bcc'].str.contains(to_))))]['body']\n",
    "            \n",
    "            if len(combo_s_r) > 100:\n",
    "                combo_s_r = combo_s_r[:100]\n",
    "            \n",
    "            \n",
    "            print('{}% done Time: {}, From: {}, To: {}. Entries: {}                            '.format((round(index/\n",
    "                                                            len(combo_list),4)*100),\n",
    "                                                            time.strftime(\"%I:%M:%S\"), from_, to_,len(combo_s_r)), end=\"\\r\")\n",
    "\n",
    "            #calculate mean tf_idf score\n",
    "            tf_idf_mean = tfidf(combo_s_r, tfidfv)\n",
    "\n",
    "            #append score to list\n",
    "            tf_idf_body.append(tf_idf_mean)\n",
    "            \n",
    "    return tf_idf_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing output of code with parallel processing\n",
    "testsample = list_of_s_r[list_of_s_r['sender'] == 'leslie.hansen@enron.com']\n",
    "print('Starting {}'.format(time.strftime(\"%I:%M:%S\")))\n",
    "scores = add_tf_score_v2(testsample, wanted_emails)\n",
    "print('Finished {}'.format(time.strftime(\"%I:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
